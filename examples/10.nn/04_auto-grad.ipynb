{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://github.com/maticvl/dataHacker/tree/master/pyTorch\" target=\"_parent\">This example extends the examples at this link</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rte1f13L-qcW"
      },
      "source": [
        "### Computation graphs and Autograd in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "sV_NWJXPS3O_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "torch.set_printoptions(precision=4, sci_mode=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch's autograd (automatic differentiation) is a powerful tool that enables automatic computation of gradients for tensors. Gradients are essential for optimizing neural networks using techniques gradient descent. Autograd tracks the operations performed on tensors during forward pass and automatically computes the gradients of the output with respect to the input during the backward pass."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iX__NJIIF1a"
      },
      "source": [
        "##  Gradients without autograd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f(3) =  9\n"
          ]
        }
      ],
      "source": [
        "# Define function f(x) that returns x^2\n",
        "def f(x):\n",
        "    return x ** 2\n",
        "\n",
        "# Test the function f(x)\n",
        "print(\"f(3) = \", f(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df(3) =  6\n"
          ]
        }
      ],
      "source": [
        "# Derivative without autograd\n",
        "# Derivative of f(x) = x^2 is df(x) = 2x\n",
        "def df(x):\n",
        "    return 2 * x\n",
        "\n",
        "# Test the function df(x)\n",
        "print(\"df(3) = \", df(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_min = 0.0000, f(x_min) =0.0000\n"
          ]
        }
      ],
      "source": [
        "# Find the minimum of f(x) using gradient descent\n",
        "x_min = 10 # Initial guess\n",
        "learning_rate = 0.1\n",
        "n_iter = 100\n",
        "for i in range(n_iter):\n",
        "    x_min -= learning_rate * df(x_min)\n",
        "\n",
        "print(f\"x_min = {x_min:.4f}, f(x_min) ={f(x_min):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Gradients with autograd\n",
        "Let's now see a simple example of how the derivative is calculated. We will create a scalar tensor `x` and set the `requires_grad` parameter to `True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x tensor(3., requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# Derivative with autograd\n",
        "# Define a tensor with requires_grad=True to enable tracking of operations for gradient computation\n",
        "x = torch.tensor(3., requires_grad=True)\n",
        "print(\"x\", x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y =  tensor(9., grad_fn=<PowBackward0>)\n",
            "Gradient of y with respect to x: tensor(12.)\n"
          ]
        }
      ],
      "source": [
        "# Define a simple mathematical operation\n",
        "y = f(x) # y = x^2\n",
        "print(\"y = \", y)\n",
        "\n",
        "# Perform a backward pass to compute the gradient of y with respect to x\n",
        "# We call the backward() method on the output tensor y\n",
        "# to initiate the computation of gradients\n",
        "y.backward()\n",
        "\n",
        "# Access the gradient computed for x\n",
        "# The gradient represent the rate of change of the y with respect to x\n",
        "gradient = x.grad\n",
        "print(\"Gradient of y with respect to x:\", gradient)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwk1w6AoVSk8"
      },
      "source": [
        "## Another example of computing gradients using autograd\n",
        "We will calculate `y` the following way:\\\n",
        "$ y = 3x^2 + 4x + 2$\n",
        "\n",
        "Now let's see what we get for `x` = `3`:  \n",
        "$ y = 3(3)^2 + 4(3) + 2 $\\\n",
        "$ y = 3*9 + 12 + 2$\\\n",
        "$ y = 27 + 12 + 2 $\\\n",
        "$ y = 41 $\n",
        "\\\n",
        "The derivative of `y` with respect to the variable `x`:\\\n",
        "$\\frac{dy}{dx} = 2*3x + 4 = 6x + 4$\\\n",
        "For `x` = `3`, we get the following:\n",
        "$\\frac{dy}{dx} = 6x + 4 = 6(3) + 4 = 18 + 4 = 22$\\\n",
        "So the gradient is equal to $22$\\\n",
        "Let's see how we can do this in code using PyTorch autograd."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "-DRxYu1cB7Zc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y = 3x^2 + 4x + 2 =  41.0\n"
          ]
        }
      ],
      "source": [
        "y = 3*x**2 + 4*x + 2\n",
        "print(\"y = 3x^2 + 4x + 2 = \", y.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Call `y.backward()` to calculate the derivative for that function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "NDV33SPOvR14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "derivative of `y` with respect to `x` = tensor(34.)\n"
          ]
        }
      ],
      "source": [
        "# Compute the derivative of `y` with respect to `x`\n",
        "y.backward()\n",
        "print(\"derivative of `y` with respect to `x` =\", x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGA2TTZ2a72l"
      },
      "source": [
        "### To turn off the gradient calculation, we can use requires_grad_(false) method or detach() method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "xlL9kf-UbEaQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3., requires_grad=True)\n",
            "tensor(3.)\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor(3., requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "x = x.requires_grad_(False)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "FSQmsP8mVgfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(3.)\n"
          ]
        }
      ],
      "source": [
        "x = x.detach()\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFiwtMAiWX9_"
      },
      "source": [
        "# Gradient accumulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUcL2csoPiov"
      },
      "source": [
        "The auto gradient calculation does not reset the gradients automatically, therefore we have to reset them after each optimization. If we forget this step they could end up just accumulating.\\\n",
        "To reset the gradients for a particular tensor, you can simply call `x.grad.zero_()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "vxLVo9P_WS2G"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(22.)\n",
            "tensor(44.)\n",
            "tensor(66.)\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor(3., requires_grad=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "  y = 3*x**2 + 4*x + 2\n",
        "  # auto gradient calculation does not reset the gradients automatically, \n",
        "  # gradients end up just accumulating\n",
        "  y.backward()\n",
        "  print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To reset the gradients for a particular tensor, you can simply call `x.grad.zero_()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(22.)\n",
            "tensor(22.)\n",
            "tensor(22.)\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor(3., requires_grad=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "  y = 3*x**2 + 4*x + 2\n",
        "  y.backward()\n",
        "  print(x.grad)\n",
        "  x.grad.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Gradients for multiple variables without autograd\n",
        "Let's enhance the example with two input variables in the neural network. We'll use two input features (x1 and x2), two corresponding weights (w1 and w2), and one bias term (b). The output y is calculated as follows: $ y = w1*x1 + w2*x2 + b$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The gradient is defined as:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} ( \\hat{y}^{(i)} - y^{(i)})x^{(i)} \\tag{1}\\\\\n",
        "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (\\hat{y}^{(i)} - y^{(i)}) \\tag{2}\\\\\n",
        "\\end{align}\n",
        "$$\n",
        "Where:\n",
        "* m is the number of training examples in the data set\n",
        "\n",
        "* $\\hat{y}^{(i)} = f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x1: tensor([2., 3.])\n",
            "x2: tensor([2., 1.])\n",
            "Gradients of loss with respect to y_pred: tensor([-2., -4.])\n",
            "Gradients of loss with respect to w1: tensor(-16.)\n",
            "Gradients of loss with respect to w2: tensor(-8.)\n",
            "Gradient of loss with respect to b: -6.0\n"
          ]
        }
      ],
      "source": [
        "# Define input features (x1 and x2)\n",
        "x1 = torch.tensor([2, 3], dtype=torch.float32)\n",
        "x2 = torch.tensor([2, 1], dtype=torch.float32)\n",
        "\n",
        "print(\"x1:\", x1)\n",
        "print(\"x2:\", x2)\n",
        "\n",
        "# Define weights and bias\n",
        "w1 = torch.tensor([3, 2], dtype=torch.float32)\n",
        "w2 = torch.tensor([1, 4], dtype=torch.float32)\n",
        "b = torch.tensor(1, dtype=torch.float32)\n",
        "\n",
        "# Define the forward pass of a simple neural network\n",
        "y_pred = x1 * w1 + x2 * w2 + b\n",
        "\n",
        "# Define a target value (ground truth)\n",
        "y_true = torch.tensor([11, 15], dtype=torch.float32)\n",
        "\n",
        "# Compute the loss manually\n",
        "loss = torch.mean((y_pred - y_true).pow(2))\n",
        "\n",
        "# Compute gradients manually using chain rule\n",
        "# dloss/dy_pred : Derivative of loss with respect to y_pred\n",
        "dloss_dy_pred = 2 * (y_pred - y_true) / y_pred.numel()  # .numel() returns the number of elements in the tensor \n",
        "# Derivative of loss with respect to w1, w2, and b\n",
        "\n",
        "# dloss/dw1 = dloss/dy_pred * dy_pred/dw1 = dloss/dy_pred * x1\n",
        "dloss_dw1 = (dloss_dy_pred * x1).sum()\n",
        "\n",
        "# dloss/dw2 = dloss/dy_pred * dy_pred/dw2 = dloss/dy_pred * x2  \n",
        "dloss_dw2 = (dloss_dy_pred * x2).sum()\n",
        "\n",
        "# dloss/db = dloss/dy_pred * dy_pred/db = dloss/dy_pred * 1\n",
        "dloss_db = dloss_dy_pred.sum()\n",
        "\n",
        "# Print the computed gradients\n",
        "print(\"Gradients of loss with respect to y_pred:\", dloss_dy_pred)\n",
        "print(\"Gradients of loss with respect to w1:\", dloss_dw1)\n",
        "print(\"Gradients of loss with respect to w2:\", dloss_dw2)\n",
        "print(\"Gradient of loss with respect to b:\", dloss_db.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Gradients for multiple variables with autograd\n",
        "We'll use two input features (x1 and x2), two corresponding weights (w1 and w2), and one bias term (b) to calculate the output y as follows: $ y = w1*x1 + w2*x2 + b$ \\\n",
        "We'll perform a forward pass, compute the loss, and then compute the gradients of the loss with respect to the weights and bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x1: tensor([2., 3.])\n",
            "x2: tensor([2., 1.])\n",
            "w1: tensor([3., 2.], requires_grad=True)\n",
            "w2: tensor([1., 4.], requires_grad=True)\n",
            "b: tensor(1., requires_grad=True)\n",
            "y_pred: tensor([ 9., 11.], grad_fn=<AddBackward0>)\n",
            "y_true: tensor([11., 15.])\n",
            "loss: tensor(10., grad_fn=<MeanBackward0>)\n",
            "Gradients of loss with respect to w1: tensor(-16.)\n",
            "Gradients of loss with respect to w2: tensor(-8.)\n",
            "Gradient of loss with respect to b: tensor(-6.)\n"
          ]
        }
      ],
      "source": [
        "# Define input features (x1 and x2) as tensors \n",
        "x1 = torch.tensor([2, 3], dtype=torch.float32)\n",
        "x2 = torch.tensor([2, 1], dtype=torch.float32)\n",
        "\n",
        "print(\"x1:\", x1)\n",
        "print(\"x2:\", x2)\n",
        "\n",
        "# Define weights and bias\n",
        "# Set requires_grad=True to compute gradients with respect to these tensors\n",
        "w1 = torch.tensor([3, 2], dtype=torch.float32, requires_grad=True)\n",
        "w2 = torch.tensor([1, 4], dtype=torch.float32, requires_grad=True)\n",
        "b = torch.tensor(1, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "print(\"w1:\", w1)\n",
        "print(\"w2:\", w2)\n",
        "print(\"b:\", b)\n",
        "\n",
        "# Define the forward pass of a simple neural network\n",
        "# to compute the predicted output y_pred using the defined weights, \n",
        "# input features, and bias term\n",
        "y_pred = x1 * w1 + x2 * w2 + b\n",
        "print(\"y_pred:\", y_pred)\n",
        "\n",
        "# Define a target value (ground truth)\n",
        "y_true = torch.tensor([11, 15], dtype=torch.float32)\n",
        "print(\"y_true:\", y_true)\n",
        "\n",
        "# Define a loss function (mean squared error)\n",
        "# between the predicted output y_pred and the target y_true\n",
        "loss = torch.mean((y_pred - y_true).pow(2))\n",
        "print(\"loss:\", loss)\n",
        "\n",
        "# Perform a backward pass to compute the gradients of loss with respect to \n",
        "# w1 and w2, and b\n",
        "loss.backward()\n",
        "\n",
        "# Access the gradients computed for w1, w2, and b\n",
        "# The gradients represent the rate of change of the loss with respect to w1, w2, and b\n",
        "gradient_w1 = w1.grad\n",
        "gradient_w2 = w2.grad\n",
        "gradient_b = b.grad\n",
        "\n",
        "# Print the gradients\n",
        "print(\"Gradients of loss with respect to w1:\", gradient_w1.sum())\n",
        "print(\"Gradients of loss with respect to w2:\", gradient_w2.sum())\n",
        "print(\"Gradient of loss with respect to b:\", gradient_b)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "004_autoGrad.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
